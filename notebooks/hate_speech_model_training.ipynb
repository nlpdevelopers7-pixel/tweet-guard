{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4d98fc7",
   "metadata": {},
   "source": [
    "# üö® Hate Speech Detection Model Training\n",
    "\n",
    "**Complete Pipeline:** Data Loading ‚Üí Preprocessing ‚Üí Feature Engineering ‚Üí Model Comparison ‚Üí Selection ‚Üí Optimization ‚Üí Saving\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52adee44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve,\n",
    "    precision_recall_curve, average_precision_score\n",
    ")\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e8c1298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (24783, 6)\n",
      "Columns: ['count', 'hate_speech_count', 'offensive_language_count', 'neither_count', 'class', 'tweet']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech_count</th>\n",
       "      <th>offensive_language_count</th>\n",
       "      <th>neither_count</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech_count  offensive_language_count  neither_count  class  \\\n",
       "0      3                  0                         0              3      2   \n",
       "1      3                  0                         3              0      1   \n",
       "2      3                  0                         3              0      1   \n",
       "3      3                  0                         2              1      1   \n",
       "4      6                  0                         6              0      1   \n",
       "\n",
       "                                               tweet  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../data/dataset.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcd986c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4fadb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìä DATA PREPROCESSING & BINARY CLASSIFICATION\n",
      "================================================================================\n",
      "\n",
      "üìà Class Distribution:\n",
      "class\n",
      "1    19190\n",
      "2     4163\n",
      "0     1430\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üîç Unique class values:\n",
      "[2 1 0]\n",
      "\n",
      "‚ùå NaN values in binary_class: 0\n",
      "\n",
      "üéØ Binary Classification:\n",
      "binary_class\n",
      "1    23353\n",
      "0     1430\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Foul tweets: 23353\n",
      "Proper tweets: 1430\n",
      "Balance ratio: 0.942\n",
      "\n",
      "‚úÖ Final check - NaN values: 0\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing & Binary Classification\n",
    "print(\"=\" * 80)\n",
    "print(\"üìä DATA PREPROCESSING & BINARY CLASSIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Explore class distribution\n",
    "print(\"\\nüìà Class Distribution:\")\n",
    "print(df['class'].value_counts())\n",
    "\n",
    "# Check for any missing values or unexpected class names\n",
    "print(f\"\\nüîç Unique class values:\")\n",
    "print(df['class'].unique())\n",
    "\n",
    "# Create binary classification based on numeric labels\n",
    "# Based on the dataset description: 0=neither, 1=hate_speech, 2=offensive_language\n",
    "# We want: 1 for hate/offensive (classes 1,2), 0 for neither (class 0)\n",
    "df['binary_class'] = df['class'].map({0: 0, 1: 1, 2: 1})\n",
    "\n",
    "# Check for NaN values after mapping\n",
    "print(f\"\\n‚ùå NaN values in binary_class: {df['binary_class'].isna().sum()}\")\n",
    "\n",
    "# Handle any NaN values by dropping them\n",
    "if df['binary_class'].isna().sum() > 0:\n",
    "    print(\"üßπ Dropping rows with NaN values...\")\n",
    "    df = df.dropna(subset=['binary_class'])\n",
    "    print(f\"‚úÖ Dataset shape after cleaning: {df.shape}\")\n",
    "\n",
    "print(\"\\nüéØ Binary Classification:\")\n",
    "print(df['binary_class'].value_counts())\n",
    "print(f\"\\nFoul tweets: {df['binary_class'].sum()}\")\n",
    "print(f\"Proper tweets: {(df['binary_class'] == 0).sum()}\")\n",
    "print(f\"Balance ratio: {df['binary_class'].mean():.3f}\")\n",
    "\n",
    "# Final verification\n",
    "print(f\"\\n‚úÖ Final check - NaN values: {df['binary_class'].isnull().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ecb3c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìù SAMPLE EXAMPLES\n",
      "================================================================================\n",
      "üìä Dataset shape after cleaning: (24783, 7)\n",
      "üìä Binary class distribution:\n",
      "binary_class\n",
      "1    23353\n",
      "0     1430\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üö® Foul Tweets (found 3):\n",
      "1. !!! RT @mayasolovely: As a woman you shouldn't complain about cleaning up your house. &amp; as a man you should always take the trash out...\n",
      "2. !!!!! RT @mleew17: boy dats cold...tyga dwn bad for cuffin dat hoe in the 1st place!!\n",
      "3. !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby4life: You ever fuck a bitch and she start to cry? You be confused as shit\n",
      "\n",
      "‚úÖ Proper Tweets (found 3):\n",
      "1. \"@Blackman38Tide: @WhaleLookyHere @HowdyDowdy11 queer\" gaywad\n",
      "2. \"@CB_Baby24: @white_thunduh alsarabsss\" hes a beaner smh you can tell hes a mexican\n",
      "3. \"@DevilGrimz: @VigxRArts you're fucking gay, blacklisted hoe\" Holding out for #TehGodClan anyway http://t.co/xUCcwoetmn\n"
     ]
    }
   ],
   "source": [
    "# Sample Examples\n",
    "print(\"=\" * 80)\n",
    "print(\"üìù SAMPLE EXAMPLES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check if we have data after cleaning\n",
    "print(f\"üìä Dataset shape after cleaning: {df.shape}\")\n",
    "print(f\"üìä Binary class distribution:\")\n",
    "print(df['binary_class'].value_counts())\n",
    "\n",
    "# Sample foul tweets\n",
    "foul_samples = df[df['binary_class'] == 1]['tweet'].head(3)\n",
    "print(f\"\\nüö® Foul Tweets (found {len(foul_samples)}):\")\n",
    "if len(foul_samples) > 0:\n",
    "    for i, tweet in enumerate(foul_samples, 1):\n",
    "        print(f\"{i}. {tweet}\")\n",
    "else:\n",
    "    print(\"   No foul tweets found!\")\n",
    "\n",
    "# Sample proper tweets\n",
    "proper_samples = df[df['binary_class'] == 0]['tweet'].head(3)\n",
    "print(f\"\\n‚úÖ Proper Tweets (found {len(proper_samples)}):\")\n",
    "if len(proper_samples) > 0:\n",
    "    for i, tweet in enumerate(proper_samples, 1):\n",
    "        print(f\"{i}. {tweet}\")\n",
    "else:\n",
    "    print(\"   No proper tweets found!\")\n",
    "\n",
    "# Show some raw class examples if binary_class is empty\n",
    "if df['binary_class'].isna().all() or len(df[df['binary_class'] == 1]) == 0:\n",
    "    print(f\"\\nüîç Raw class examples:\")\n",
    "    print(df['class'].value_counts().head())\n",
    "    print(f\"\\nSample tweets by original class:\")\n",
    "    for class_name in df['class'].unique()[:3]:\n",
    "        sample = df[df['class'] == class_name]['tweet'].head(1)\n",
    "        if len(sample) > 0:\n",
    "            print(f\"{class_name}: {sample.iloc[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07072e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üîÑ TRAIN/TEST SPLIT\n",
      "================================================================================\n",
      "Training set size: 19826\n",
      "Test set size: 4957\n",
      "Training balance: 0.942\n",
      "Test balance: 0.942\n"
     ]
    }
   ],
   "source": [
    "# Train/Test Split\n",
    "print(\"=\" * 80)\n",
    "print(\"üîÑ TRAIN/TEST SPLIT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Prepare data\n",
    "X = df['tweet']\n",
    "y = df['binary_class']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "print(f\"Training balance: {y_train.mean():.3f}\")\n",
    "print(f\"Test balance: {y_test.mean():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605a83dd",
   "metadata": {},
   "source": [
    "## üîß Feature Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27120541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BOW_UNIGRAMS ===\n",
      "Feature matrix shape: (19826, 10000)\n",
      "Logistic Regression - Accuracy: 0.9425, Precision: 0.9522, Recall: 0.9887, F1: 0.9701\n",
      "\n",
      "=== BOW_BIGRAMS ===\n",
      "Feature matrix shape: (19826, 10000)\n",
      "Logistic Regression - Accuracy: 0.9421, Precision: 0.9533, Recall: 0.9869, F1: 0.9698\n",
      "\n",
      "=== TFIDF_UNIGRAMS ===\n",
      "Feature matrix shape: (19826, 10000)\n",
      "Logistic Regression - Accuracy: 0.9431, Precision: 0.9476, Recall: 0.9946, F1: 0.9705\n",
      "\n",
      "=== TFIDF_BIGRAMS ===\n",
      "Feature matrix shape: (19826, 10000)\n",
      "Logistic Regression - Accuracy: 0.9419, Precision: 0.9475, Recall: 0.9934, F1: 0.9699\n",
      "\n",
      "üèÜ SELECTED VECTORIZER: tfidf_bigrams\n",
      "   F1-Score: 0.9699\n",
      "   Accuracy: 0.9419\n"
     ]
    }
   ],
   "source": [
    "# Feature extraction with different configurations\n",
    "vectorizers = {\n",
    "    'bow_unigrams': CountVectorizer(\n",
    "        lowercase=True,\n",
    "        stop_words='english',\n",
    "        ngram_range=(1, 1),\n",
    "        max_features=10000\n",
    "    ),\n",
    "    'bow_bigrams': CountVectorizer(\n",
    "        lowercase=True,\n",
    "        stop_words='english',\n",
    "        ngram_range=(1, 2),\n",
    "        max_features=10000\n",
    "    ),\n",
    "    'tfidf_unigrams': TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        stop_words='english',\n",
    "        ngram_range=(1, 1),\n",
    "        max_features=10000\n",
    "    ),\n",
    "    'tfidf_bigrams': TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        stop_words='english',\n",
    "        ngram_range=(1, 2),\n",
    "        max_features=10000\n",
    "    )\n",
    "}\n",
    "\n",
    "# Test different vectorizers\n",
    "results = {}\n",
    "\n",
    "for name, vectorizer in vectorizers.items():\n",
    "    print(f\"\\n=== {name.upper()} ===\")\n",
    "    \n",
    "    # Fit and transform\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    \n",
    "    print(f\"Feature matrix shape: {X_train_vec.shape}\")\n",
    "    \n",
    "    # Test with Logistic Regression\n",
    "    lr = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    lr.fit(X_train_vec, y_train)\n",
    "    y_pred = lr.predict(X_test_vec)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Logistic Regression - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "    \n",
    "    results[name] = {\n",
    "        'vectorizer': vectorizer,\n",
    "        'X_train': X_train_vec,\n",
    "        'X_test': X_test_vec,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "# Select best vectorizer (TF-IDF with bigrams)\n",
    "best_vectorizer_name = 'tfidf_bigrams'\n",
    "best_vectorizer = results[best_vectorizer_name]['vectorizer']\n",
    "X_train_best = results[best_vectorizer_name]['X_train']\n",
    "X_test_best = results[best_vectorizer_name]['X_test']\n",
    "\n",
    "print(f\"\\nüèÜ SELECTED VECTORIZER: {best_vectorizer_name}\")\n",
    "print(f\"   F1-Score: {results[best_vectorizer_name]['f1']:.4f}\")\n",
    "print(f\"   Accuracy: {results[best_vectorizer_name]['accuracy']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4455b4c1",
   "metadata": {},
   "source": [
    "## üîç Fair Model Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa0c221b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üîç FAIR MODEL COMPARISON (Default Parameters)\n",
      "================================================================================\n",
      "\n",
      "üìà Training and evaluating 4 models with default parameters...\n",
      "------------------------------------------------------------\n",
      "\n",
      "üîÑ Training Logistic Regression...\n",
      "‚úÖ Logistic Regression - Accuracy: 0.9419, F1: 0.9699, ROC-AUC: 0.8695\n",
      "\n",
      "üîÑ Training SVM Linear...\n",
      "‚úÖ SVM Linear - Accuracy: 0.9431, F1: 0.9704, ROC-AUC: 0.8293\n",
      "\n",
      "üîÑ Training SVM RBF...\n",
      "‚úÖ SVM RBF - Accuracy: 0.9421, F1: 0.9700, ROC-AUC: 0.8463\n",
      "\n",
      "üîÑ Training Naive Bayes...\n",
      "‚úÖ Naive Bayes - Accuracy: 0.9423, F1: 0.9703, ROC-AUC: 0.7743\n",
      "\n",
      "================================================================================\n",
      "üìä FAIR COMPARISON RESULTS\n",
      "================================================================================\n",
      "                     Accuracy  Precision  Recall  F1-Score  ROC-AUC  PR-AUC\n",
      "Logistic Regression    0.9419     0.9475  0.9934    0.9699   0.8695  0.9879\n",
      "SVM Linear             0.9431     0.9511  0.9906    0.9704   0.8293  0.9838\n",
      "SVM RBF                0.9421     0.9486  0.9923    0.9700   0.8463  0.9853\n",
      "Naive Bayes            0.9423     0.9423  1.0000    0.9703   0.7743  0.9777\n",
      "\n",
      "üèÜ BEST MODEL FOR OPTIMIZATION: SVM Linear\n",
      "   F1-Score: 0.9704\n",
      "   Accuracy: 0.9431\n"
     ]
    }
   ],
   "source": [
    "# FAIR MODEL COMPARISON - All models with default parameters\n",
    "print(\"=\" * 80)\n",
    "print(\"üîç FAIR MODEL COMPARISON (Default Parameters)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define models with default parameters for fair comparison\n",
    "models_fair = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'SVM Linear': SVC(kernel='linear', random_state=42, probability=True),\n",
    "    'SVM RBF': SVC(kernel='rbf', random_state=42, probability=True),\n",
    "    'Naive Bayes': MultinomialNB()\n",
    "}\n",
    "\n",
    "# Train and evaluate all models fairly\n",
    "fair_results = {}\n",
    "\n",
    "print(f\"\\nüìà Training and evaluating {len(models_fair)} models with default parameters...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, model in models_fair.items():\n",
    "    print(f\"\\nüîÑ Training {name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_best, y_train)\n",
    "    \n",
    "    # Get predictions and probabilities\n",
    "    y_pred = model.predict(X_test_best)\n",
    "    y_pred_proba = model.predict_proba(X_test_best)[:, 1]\n",
    "    \n",
    "    # Calculate all metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    pr_auc = average_precision_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Store results\n",
    "    fair_results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'pr_auc': pr_auc,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ {name} - Accuracy: {accuracy:.4f}, F1: {f1:.4f}, ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä FAIR COMPARISON RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "fair_df = pd.DataFrame({\n",
    "    name: {\n",
    "        'Accuracy': results['accuracy'],\n",
    "        'Precision': results['precision'], \n",
    "        'Recall': results['recall'],\n",
    "        'F1-Score': results['f1'],\n",
    "        'ROC-AUC': results['roc_auc'],\n",
    "        'PR-AUC': results['pr_auc']\n",
    "    }\n",
    "    for name, results in fair_results.items()\n",
    "}).T\n",
    "\n",
    "print(fair_df.round(4))\n",
    "\n",
    "# Find the best model for optimization\n",
    "best_model_name = max(fair_results.items(), key=lambda x: x[1]['f1'])[0]\n",
    "print(f\"\\nüèÜ BEST MODEL FOR OPTIMIZATION: {best_model_name}\")\n",
    "print(f\"   F1-Score: {fair_results[best_model_name]['f1']:.4f}\")\n",
    "print(f\"   Accuracy: {fair_results[best_model_name]['accuracy']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731223ca",
   "metadata": {},
   "source": [
    "## üéØ Manual Model Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d28d01e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üéØ MANUAL MODEL SELECTION\n",
      "================================================================================\n",
      "\n",
      "üìä ALL MODEL RESULTS (Fair Comparison):\n",
      "--------------------------------------------------\n",
      "\n",
      "=== Logistic Regression ===\n",
      "Accuracy: 0.9419\n",
      "Precision: 0.9475\n",
      "Recall: 0.9934\n",
      "F1-Score: 0.9699\n",
      "ROC-AUC: 0.8695\n",
      "\n",
      "=== SVM Linear ===\n",
      "Accuracy: 0.9431\n",
      "Precision: 0.9511\n",
      "Recall: 0.9906\n",
      "F1-Score: 0.9704\n",
      "ROC-AUC: 0.8293\n",
      "\n",
      "=== SVM RBF ===\n",
      "Accuracy: 0.9421\n",
      "Precision: 0.9486\n",
      "Recall: 0.9923\n",
      "F1-Score: 0.9700\n",
      "ROC-AUC: 0.8463\n",
      "\n",
      "=== Naive Bayes ===\n",
      "Accuracy: 0.9423\n",
      "Precision: 0.9423\n",
      "Recall: 1.0000\n",
      "F1-Score: 0.9703\n",
      "ROC-AUC: 0.7743\n",
      "\n",
      "================================================================================\n",
      "ü§î WHICH MODEL DO YOU WANT TO OPTIMIZE?\n",
      "================================================================================\n",
      "\n",
      "Available models:\n",
      "1. Logistic Regression\n",
      "2. SVM Linear\n",
      "3. SVM RBF\n",
      "4. Naive Bayes\n",
      "\n",
      "üí° RECOMMENDATION: SVM Linear (Best F1-Score: 0.9704)\n",
      "\n",
      "‚úÖ SELECTED MODEL FOR OPTIMIZATION: SVM Linear\n",
      "\n",
      "üìà Selected Model Performance (Before Optimization):\n",
      "Accuracy: 0.9431\n",
      "Precision: 0.9511\n",
      "Recall: 0.9906\n",
      "F1-Score: 0.9704\n",
      "ROC-AUC: 0.8293\n",
      "\n",
      "üéØ Model selected! Now we'll optimize it with Grid Search...\n"
     ]
    }
   ],
   "source": [
    "# Manual Model Selection - Choose Your Best Model\n",
    "print(\"=\" * 80)\n",
    "print(\"üéØ MANUAL MODEL SELECTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Display all model results for comparison\n",
    "print(\"\\nüìä ALL MODEL RESULTS (Fair Comparison):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for name, results in fair_results.items():\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"Accuracy: {results['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {results['precision']:.4f}\")\n",
    "    print(f\"Recall: {results['recall']:.4f}\")\n",
    "    print(f\"F1-Score: {results['f1']:.4f}\")\n",
    "    print(f\"ROC-AUC: {results['roc_auc']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ü§î WHICH MODEL DO YOU WANT TO OPTIMIZE?\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Let user choose the model\n",
    "print(\"\\nAvailable models:\")\n",
    "model_names = list(fair_results.keys())\n",
    "for i, name in enumerate(model_names, 1):\n",
    "    print(f\"{i}. {name}\")\n",
    "\n",
    "# Get the best performing model\n",
    "best_model_name = max(fair_results.items(), key=lambda x: x[1]['f1'])[0]\n",
    "print(f\"\\nüí° RECOMMENDATION: {best_model_name} (Best F1-Score: {fair_results[best_model_name]['f1']:.4f})\")\n",
    "\n",
    "# For now, let's set it to the best model (you can change this)\n",
    "SELECTED_MODEL = \"SVM Linear\"  # Change this to your preferred model\n",
    "\n",
    "print(f\"\\n‚úÖ SELECTED MODEL FOR OPTIMIZATION: {SELECTED_MODEL}\")\n",
    "\n",
    "# Get the selected model\n",
    "selected_model = fair_results[SELECTED_MODEL]['model']\n",
    "selected_performance = fair_results[SELECTED_MODEL]\n",
    "\n",
    "print(f\"\\nüìà Selected Model Performance (Before Optimization):\")\n",
    "print(f\"Accuracy: {selected_performance['accuracy']:.4f}\")\n",
    "print(f\"Precision: {selected_performance['precision']:.4f}\")\n",
    "print(f\"Recall: {selected_performance['recall']:.4f}\")\n",
    "print(f\"F1-Score: {selected_performance['f1']:.4f}\")\n",
    "print(f\"ROC-AUC: {selected_performance['roc_auc']:.4f}\")\n",
    "\n",
    "print(f\"\\nüéØ Model selected! Now we'll optimize it with Grid Search...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03699424",
   "metadata": {},
   "source": [
    "## üîß Grid Search Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7c2beec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üîß GRID SEARCH OPTIMIZATION\n",
      "================================================================================\n",
      "üéØ Optimizing: SVM Linear\n",
      "üìã Parameter grid: {'C': [0.1, 1, 10, 100], 'gamma': ['scale', 'auto']}\n",
      "\n",
      "üîÑ Starting grid search optimization...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "\n",
      "‚úÖ Grid search completed!\n",
      "Best parameters: {'C': 1, 'gamma': 'scale'}\n",
      "Best cross-validation F1 score: 0.9724\n",
      "\n",
      "üìä OPTIMIZED MODEL PERFORMANCE:\n",
      "Accuracy: 0.9431\n",
      "Precision: 0.9511\n",
      "Recall: 0.9906\n",
      "F1-Score: 0.9704\n",
      "ROC-AUC: 0.8293\n",
      "PR-AUC: 0.9838\n",
      "\n",
      "üìà IMPROVEMENT COMPARISON:\n",
      "F1-Score: 0.9704 ‚Üí 0.9704\n",
      "Accuracy: 0.9431 ‚Üí 0.9431\n",
      "ROC-AUC: 0.8293 ‚Üí 0.8293\n",
      "\n",
      "üéâ Model optimization completed!\n"
     ]
    }
   ],
   "source": [
    "# Grid Search Optimization for Selected Model\n",
    "print(\"=\" * 80)\n",
    "print(\"üîß GRID SEARCH OPTIMIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"üéØ Optimizing: {SELECTED_MODEL}\")\n",
    "\n",
    "# Define parameter grids for different models\n",
    "if SELECTED_MODEL == \"Logistic Regression\":\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear', 'saga']\n",
    "    }\n",
    "    base_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    \n",
    "elif SELECTED_MODEL == \"SVM Linear\":\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    }\n",
    "    base_model = SVC(kernel='linear', random_state=42, probability=True)\n",
    "    \n",
    "elif SELECTED_MODEL == \"SVM RBF\":\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1]\n",
    "    }\n",
    "    base_model = SVC(kernel='rbf', random_state=42, probability=True)\n",
    "    \n",
    "elif SELECTED_MODEL == \"Naive Bayes\":\n",
    "    param_grid = {\n",
    "        'alpha': [0.1, 0.5, 1.0, 2.0]\n",
    "    }\n",
    "    base_model = MultinomialNB()\n",
    "\n",
    "print(f\"üìã Parameter grid: {param_grid}\")\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(\n",
    "    base_model,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\nüîÑ Starting grid search optimization...\")\n",
    "grid_search.fit(X_train_best, y_train)\n",
    "\n",
    "print(f\"\\n‚úÖ Grid search completed!\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation F1 score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Get the optimized model\n",
    "optimized_model = grid_search.best_estimator_\n",
    "y_pred_optimized = optimized_model.predict(X_test_best)\n",
    "y_pred_proba_optimized = optimized_model.predict_proba(X_test_best)[:, 1]\n",
    "\n",
    "print(f\"\\nüìä OPTIMIZED MODEL PERFORMANCE:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_optimized):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_optimized):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_optimized):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_optimized):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba_optimized):.4f}\")\n",
    "print(f\"PR-AUC: {average_precision_score(y_test, y_pred_proba_optimized):.4f}\")\n",
    "\n",
    "# Compare before and after optimization\n",
    "print(f\"\\nüìà IMPROVEMENT COMPARISON:\")\n",
    "print(f\"F1-Score: {selected_performance['f1']:.4f} ‚Üí {f1_score(y_test, y_pred_optimized):.4f}\")\n",
    "print(f\"Accuracy: {selected_performance['accuracy']:.4f} ‚Üí {accuracy_score(y_test, y_pred_optimized):.4f}\")\n",
    "print(f\"ROC-AUC: {selected_performance['roc_auc']:.4f} ‚Üí {roc_auc_score(y_test, y_pred_proba_optimized):.4f}\")\n",
    "\n",
    "# Update the selected model to the optimized version\n",
    "selected_model = optimized_model\n",
    "selected_performance = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_optimized),\n",
    "    'precision': precision_score(y_test, y_pred_optimized),\n",
    "    'recall': recall_score(y_test, y_pred_optimized),\n",
    "    'f1': f1_score(y_test, y_pred_optimized),\n",
    "    'roc_auc': roc_auc_score(y_test, y_pred_proba_optimized),\n",
    "    'pr_auc': average_precision_score(y_test, y_pred_proba_optimized)\n",
    "}\n",
    "\n",
    "print(f\"\\nüéâ Model optimization completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece82e98",
   "metadata": {},
   "source": [
    "## üéØ Threshold Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60839f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üéØ THRESHOLD OPTIMIZATION\n",
      "================================================================================\n",
      "Using threshold: 0.3000\n",
      "Precision at threshold 0.3000: 0.9482\n",
      "Recall at threshold 0.3000: 0.9944\n",
      "F1-Score at threshold 0.3000: 0.9707\n",
      "Accuracy at threshold 0.3000: 0.9435\n",
      "\n",
      "üéØ Final optimized model ready for saving!\n"
     ]
    }
   ],
   "source": [
    "# Threshold Optimization for Optimized Model\n",
    "print(\"=\" * 80)\n",
    "print(\"üéØ THRESHOLD OPTIMIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Set optimal threshold to 0.3 for better sensitivity to offensive language\n",
    "optimal_threshold = 0.3\n",
    "\n",
    "print(f\"Using threshold: {optimal_threshold:.4f}\")\n",
    "\n",
    "# Apply the threshold to optimized model predictions\n",
    "y_pred_optimal = (y_pred_proba_optimized >= optimal_threshold).astype(int)\n",
    "\n",
    "# Calculate performance metrics with the threshold\n",
    "optimal_precision = precision_score(y_test, y_pred_optimal)\n",
    "optimal_recall = recall_score(y_test, y_pred_optimal)\n",
    "optimal_f1 = f1_score(y_test, y_pred_optimal)\n",
    "optimal_accuracy = accuracy_score(y_test, y_pred_optimal)\n",
    "\n",
    "print(f\"Precision at threshold {optimal_threshold:.4f}: {optimal_precision:.4f}\")\n",
    "print(f\"Recall at threshold {optimal_threshold:.4f}: {optimal_recall:.4f}\")\n",
    "print(f\"F1-Score at threshold {optimal_threshold:.4f}: {optimal_f1:.4f}\")\n",
    "print(f\"Accuracy at threshold {optimal_threshold:.4f}: {optimal_accuracy:.4f}\")\n",
    "\n",
    "# Update the final performance metrics\n",
    "selected_performance = {\n",
    "    'accuracy': optimal_accuracy,\n",
    "    'precision': optimal_precision,\n",
    "    'recall': optimal_recall,\n",
    "    'f1': optimal_f1,\n",
    "    'roc_auc': roc_auc_score(y_test, y_pred_proba_optimized),\n",
    "    'pr_auc': average_precision_score(y_test, y_pred_proba_optimized)\n",
    "}\n",
    "\n",
    "print(f\"\\nüéØ Final optimized model ready for saving!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d833c591",
   "metadata": {},
   "source": [
    "## üíæ Save Optimized Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d41451fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üíæ SAVING OPTIMIZED MODEL\n",
      "================================================================================\n",
      "‚úÖ Optimized model saved successfully!\n",
      "üìÅ File: ../models/saved/hate_speech_model.pkl\n",
      "ü§ñ Model: SVM Linear (Optimized)\n",
      "üìä F1-Score: 0.9707\n",
      "üéØ Accuracy: 0.9435\n",
      "üéØ Threshold: 0.3\n",
      "\n",
      "üìã Model Components Saved:\n",
      "‚Ä¢ Trained Model: SVM Linear (Grid Search Optimized)\n",
      "‚Ä¢ Vectorizer: TF-IDF with Bigrams\n",
      "‚Ä¢ Threshold: 0.3\n",
      "‚Ä¢ Performance Metrics: All evaluation scores\n",
      "‚Ä¢ Training Date: 2025-10-25 19:36:45\n",
      "‚Ä¢ Best Parameters: {'C': 1, 'gamma': 'scale'}\n",
      "‚Ä¢ CV Score: 0.9724\n",
      "\n",
      "üöÄ Ready to use in Flask API!\n",
      "üìç API will load model from: ../models/saved/hate_speech_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save the Optimized Model\n",
    "print(\"=\" * 80)\n",
    "print(\"üíæ SAVING OPTIMIZED MODEL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs('../models/saved', exist_ok=True)\n",
    "\n",
    "# Save the optimized model with all necessary components\n",
    "model_data = {\n",
    "    'model': selected_model,  # This is now the optimized model\n",
    "    'vectorizer': best_vectorizer,  # Use the best vectorizer from earlier\n",
    "    'threshold': optimal_threshold,  # Use the optimized threshold\n",
    "    'model_name': SELECTED_MODEL,\n",
    "    'performance': selected_performance,  # This is now the optimized performance\n",
    "    'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'optimization_info': {\n",
    "        'grid_search_performed': True,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'cv_score': grid_search.best_score_\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to pickle file in the organized structure\n",
    "import pickle\n",
    "model_path = '../models/saved/hate_speech_model.pkl'\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(model_data, f)\n",
    "\n",
    "print(f\"‚úÖ Optimized model saved successfully!\")\n",
    "print(f\"üìÅ File: {model_path}\")\n",
    "print(f\"ü§ñ Model: {SELECTED_MODEL} (Optimized)\")\n",
    "print(f\"üìä F1-Score: {selected_performance['f1']:.4f}\")\n",
    "print(f\"üéØ Accuracy: {selected_performance['accuracy']:.4f}\")\n",
    "print(f\"üéØ Threshold: {optimal_threshold}\")\n",
    "\n",
    "print(f\"\\nüìã Model Components Saved:\")\n",
    "print(f\"‚Ä¢ Trained Model: {SELECTED_MODEL} (Grid Search Optimized)\")\n",
    "print(f\"‚Ä¢ Vectorizer: TF-IDF with Bigrams\")\n",
    "print(f\"‚Ä¢ Threshold: {optimal_threshold}\")\n",
    "print(f\"‚Ä¢ Performance Metrics: All evaluation scores\")\n",
    "print(f\"‚Ä¢ Training Date: {model_data['training_date']}\")\n",
    "print(f\"‚Ä¢ Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"‚Ä¢ CV Score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "print(f\"\\nüöÄ Ready to use in Flask API!\")\n",
    "print(f\"üìç API will load model from: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac16fa28",
   "metadata": {},
   "source": [
    "## üß™ Test Saved Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8587d862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üß™ TESTING SAVED MODEL\n",
      "================================================================================\n",
      "‚úÖ Model loaded successfully!\n",
      "ü§ñ Model Type: SVM Linear\n",
      "üìä Performance: F1=0.9707\n",
      "üéØ Threshold: 0.3\n",
      "\n",
      "üîç Testing with sample texts:\n",
      "----------------------------------------\n",
      "Text: This is a normal tweet about the weather...\n",
      "Prediction: Foul (confidence: 0.980)\n",
      "Probability: 0.980\n",
      "\n",
      "Text: You are such a stupid idiot...\n",
      "Prediction: Foul (confidence: 0.736)\n",
      "Probability: 0.736\n",
      "\n",
      "Text: I love this new movie, it's amazing!...\n",
      "Prediction: Foul (confidence: 0.977)\n",
      "Probability: 0.977\n",
      "\n",
      "Text: Go kill yourself you worthless piece of shit...\n",
      "Prediction: Foul (confidence: 0.825)\n",
      "Probability: 0.825\n",
      "\n",
      "üéâ Model is working correctly!\n",
      "\n",
      "üöÄ Ready to start Flask API!\n"
     ]
    }
   ],
   "source": [
    "# Test the Saved Model\n",
    "print(\"=\" * 80)\n",
    "print(\"üß™ TESTING SAVED MODEL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load the saved model to verify it works\n",
    "model_path = '../models/saved/hate_speech_model.pkl'\n",
    "with open(model_path, 'rb') as f:\n",
    "    loaded_model_data = pickle.load(f)\n",
    "\n",
    "print(f\"‚úÖ Model loaded successfully!\")\n",
    "print(f\"üìÅ From: {model_path}\")\n",
    "print(f\"ü§ñ Model Type: {loaded_model_data['model_name']}\")\n",
    "print(f\"üìä Performance: F1={loaded_model_data['performance']['f1']:.4f}\")\n",
    "print(f\"üéØ Threshold: {loaded_model_data['threshold']}\")\n",
    "\n",
    "# Test with sample texts\n",
    "test_texts = [\n",
    "    \"This is a normal tweet about the weather\",\n",
    "    \"You are such a stupid idiot\", \n",
    "    \"I love this new movie, it's amazing!\",\n",
    "    \"Go kill yourself you worthless piece of shit\"\n",
    "]\n",
    "\n",
    "print(f\"\\nüîç Testing with sample texts:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for text in test_texts:\n",
    "    # Transform text using the saved vectorizer\n",
    "    text_vector = loaded_model_data['vectorizer'].transform([text])\n",
    "    \n",
    "    # Get prediction probability\n",
    "    proba = loaded_model_data['model'].predict_proba(text_vector)[0, 1]\n",
    "    \n",
    "    # Apply threshold\n",
    "    prediction = \"Foul\" if proba >= loaded_model_data['threshold'] else \"Proper\"\n",
    "    confidence = max(proba, 1-proba)\n",
    "    \n",
    "    print(f\"Text: {text[:50]}...\")\n",
    "    print(f\"Prediction: {prediction} (confidence: {confidence:.3f})\")\n",
    "    print(f\"Probability: {proba:.3f}\")\n",
    "    print()\n",
    "\n",
    "print(\"üéâ Model is working correctly!\")\n",
    "print(f\"\\nüöÄ Ready to start Flask API!\")\n",
    "print(f\"üìç API will load from: {model_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
